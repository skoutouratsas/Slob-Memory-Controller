diff -ruN linux-3.14.62-orig/arch/x86/syscalls/syscall_32.tbl linux-3.14.62-dev/arch/x86/syscalls/syscall_32.tbl
--- linux-3.14.62-orig/arch/x86/syscalls/syscall_32.tbl	2016-02-25 21:59:45.000000000 +0200
+++ linux-3.14.62-dev/arch/x86/syscalls/syscall_32.tbl	2019-04-19 10:40:51.292541449 +0300
@@ -359,3 +359,5 @@
 350	i386	finit_module		sys_finit_module
 351	i386	sched_setattr		sys_sched_setattr
 352	i386	sched_getattr		sys_sched_getattr
+353     i386    slob_get_total_alloc_mem sys_slob_get_total_alloc_mem
+354     i386    slob_get_total_free_mem sys_slob_get_total_free_mem
diff -ruN linux-3.14.62-orig/include/linux/syscalls.h linux-3.14.62-dev/include/linux/syscalls.h
--- linux-3.14.62-orig/include/linux/syscalls.h	2016-02-25 21:59:45.000000000 +0200
+++ linux-3.14.62-dev/include/linux/syscalls.h	2019-04-19 10:43:56.496540543 +0300
@@ -855,4 +855,6 @@
 asmlinkage long sys_kcmp(pid_t pid1, pid_t pid2, int type,
 			 unsigned long idx1, unsigned long idx2);
 asmlinkage long sys_finit_module(int fd, const char __user *uargs, int flags);
+asmlinkage long sys_slob_get_total_alloc_mem(void);
+asmlinkage long sys_slob_get_total_free_mem(void);
 #endif
diff -ruN linux-3.14.62-orig/kernel/Makefile linux-3.14.62-dev/kernel/Makefile
--- linux-3.14.62-orig/kernel/Makefile	2016-02-25 21:59:45.000000000 +0200
+++ linux-3.14.62-dev/kernel/Makefile	2019-04-19 11:43:43.680522994 +0300
@@ -10,7 +10,7 @@
 	    kthread.o sys_ni.o posix-cpu-timers.o \
 	    hrtimer.o nsproxy.o \
 	    notifier.o ksysfs.o cred.o reboot.o \
-	    async.o range.o groups.o smpboot.o
+	    async.o range.o groups.o smpboot.o 
 
 ifdef CONFIG_FUNCTION_TRACER
 # Do not trace debug files and internal ftrace files
diff -ruN linux-3.14.62-orig/Makefile linux-3.14.62-dev/Makefile
--- linux-3.14.62-orig/Makefile	2016-02-25 21:59:45.000000000 +0200
+++ linux-3.14.62-dev/Makefile	2019-04-16 02:16:27.927353374 +0300
@@ -1,7 +1,7 @@
 VERSION = 3
 PATCHLEVEL = 14
 SUBLEVEL = 62
-EXTRAVERSION =
+EXTRAVERSION = -dev
 NAME = Remembering Coco
 
 # *DOCUMENTATION*
diff -ruN linux-3.14.62-orig/mm/Makefile linux-3.14.62-dev/mm/Makefile
--- linux-3.14.62-orig/mm/Makefile	2016-02-25 21:59:45.000000000 +0200
+++ linux-3.14.62-dev/mm/Makefile	2019-04-19 11:49:15.792521370 +0300
@@ -5,7 +5,7 @@
 mmu-y			:= nommu.o
 mmu-$(CONFIG_MMU)	:= fremap.o highmem.o madvise.o memory.o mincore.o \
 			   mlock.o mmap.o mprotect.o mremap.o msync.o rmap.o \
-			   vmalloc.o pagewalk.o pgtable-generic.o
+			   vmalloc.o pagewalk.o pgtable-generic.o 
 
 ifdef CONFIG_CROSS_MEMORY_ATTACH
 mmu-$(CONFIG_MMU)	+= process_vm_access.o
diff -ruN linux-3.14.62-orig/mm/slob.c linux-3.14.62-dev/mm/slob.c
--- linux-3.14.62-orig/mm/slob.c	2016-02-25 21:59:45.000000000 +0200
+++ linux-3.14.62-dev/mm/slob.c	2019-04-19 18:13:56.207843682 +0300
@@ -58,7 +58,7 @@
 
 #include <linux/kernel.h>
 #include <linux/slab.h>
-
+#include <linux/syscalls.h>
 #include <linux/mm.h>
 #include <linux/swap.h> /* struct reclaim_state */
 #include <linux/cache.h>
@@ -69,6 +69,7 @@
 #include <linux/kmemleak.h>
 
 #include <trace/events/kmem.h>
+#include <linux/linkage.h>
 
 #include <linux/atomic.h>
 
@@ -95,12 +96,20 @@
 /*
  * All partially free slob pages go on these lists.
  */
+ 
+//#define PAGE_BEST_FIT
+//#define BLOCK_BEST_FIT 
 #define SLOB_BREAK1 256
 #define SLOB_BREAK2 1024
 static LIST_HEAD(free_slob_small);
 static LIST_HEAD(free_slob_medium);
 static LIST_HEAD(free_slob_large);
 
+static unsigned int counter = 0;
+static long alloc_mem=0;
+static long free_mem_small=0;
+static long free_mem_medium=0;
+static long free_mem_large=0;
 /*
  * slob_page_free: true for pages on free_slob_pages list.
  */
@@ -200,7 +209,8 @@
 
 	if (!page)
 		return NULL;
-
+	
+	alloc_mem += (2^order)* PAGE_SIZE;
 	return page_address(page);
 }
 
@@ -209,11 +219,225 @@
 	if (current->reclaim_state)
 		current->reclaim_state->reclaimed_slab += 1 << order;
 	free_pages((unsigned long)b, order);
+	alloc_mem -= (2^order) * PAGE_SIZE;
 }
 
-/*
- * Allocate a slob block within a given slob_page sp.
- */
+#ifdef BLOCK_BEST_FIT
+static void *slob_page_alloc(struct page *sp, size_t size, int align)
+{
+	slob_t *prev, *cur, *aligned,*best, *best_aligned, *next,*best_prev;
+	int delta_of_best,best_avail,min, delta = 0, units = SLOB_UNITS(size);
+	
+	min = -1;
+	aligned=NULL;
+	best=NULL;
+	best_aligned=NULL;
+	best_prev=NULL;
+	
+	if (counter == 6000) 
+		printk("Slob_alloc:Request:%d\nslob_alloc:Candidate blocks size: ",units);
+	
+	for (prev = NULL, cur = sp->freelist; ; prev = cur, cur = slob_next(cur)) {
+		slobidx_t avail = slob_units(cur);
+		
+			
+		
+		if (align) {
+			aligned = (slob_t *)ALIGN((unsigned long)cur, align);
+			delta = aligned - cur;
+		}
+		if (counter == 6000)
+			printk("%d ",avail);
+		
+		if ( avail >= units + delta) { /* room enough? */
+			
+			
+			if ( min == -1 ) {  
+				min = avail - ( units + delta ); 
+				best = cur;
+				delta_of_best = delta;
+				best_aligned = aligned;
+				best_avail = avail;
+				best_prev = prev;
+			}
+			else if ( avail - ( units + delta ) < min ){
+				min = avail - ( units + delta );
+				delta_of_best = delta;
+				best_aligned = aligned;
+				best = cur;
+				best_avail = avail;
+				best_prev = prev;
+			}
+		}
+		
+		if (slob_last(cur))
+			break;
+	}
+	
+	if ( min == -1 ) {
+		if (counter == 6000) {
+			printk("\nSlob_alloc:Best Fit:None\n");
+			counter = 0;
+		}
+		return NULL; 
+	}
+	
+	best_avail = slob_units(best);
+	
+	if (delta_of_best) { /* need to fragment head to align? */
+		next = slob_next(best);
+		set_slob(best_aligned, best_avail - delta_of_best, next);
+		set_slob(best, delta_of_best, best_aligned);
+		best_prev = best;
+		best = best_aligned;
+		best_avail = slob_units(best); ////////////////////////????????????????
+	}
+
+	next = slob_next(best);
+	if (best_avail == units) { /* exact fit? unlink. */
+		if (best_prev)
+			set_slob(best_prev, slob_units(best_prev), next);
+		else
+			sp->freelist = next;
+	} else { /* fragment */
+		if (best_prev)
+			set_slob(best_prev, slob_units(best_prev), best + units);
+		else
+			sp->freelist = best + units;
+		set_slob(best + units, best_avail - units, next);
+
+	}
+	if (counter == 6000) {
+			printk("\nSlob_alloc:Best Fit:%d\n",best_avail+delta_of_best);
+			counter = 0;
+	}
+	
+	sp->units -= units;
+	if (!sp->units)
+		clear_slob_page_free(sp);
+	
+	
+	return best;
+		
+}
+#endif
+
+#ifdef PAGE_BEST_FIT
+static void *slob_alloc(size_t size, gfp_t gfp, int align, int node)
+{
+	struct page *sp,*best;
+	struct list_head *prev;
+	struct list_head *slob_list,*wall;
+	slob_t *b = NULL;
+	unsigned long flags;
+	int min,prev_min=0;
+	
+	min = 10000000;
+	counter++;
+	if (size < SLOB_BREAK1){
+		slob_list = &free_slob_small;
+		free_mem_small = 0;
+	}
+	else if (size < SLOB_BREAK2){
+		slob_list = &free_slob_medium;
+		free_mem_medium = 0;
+		
+	}
+	else{
+		slob_list = &free_slob_large;
+		free_mem_large = 0;
+	}
+
+	spin_lock_irqsave(&slob_lock, flags);
+	/* Iterate through each partially free page, try to find room */
+	
+	list_for_each_entry(sp, slob_list, list) { // get statistics, den kostizei poly
+			if (size < SLOB_BREAK1)
+				free_mem_small += sp->units;
+			else if (size < SLOB_BREAK2)
+				free_mem_medium += sp->units;
+			else
+				free_mem_large += sp->units;
+	}
+	wall = slob_list;
+	while(1){
+		if(slob_list->next == wall)break;}
+		list_for_each_entry(sp, slob_list, list) {
+			
+			
+			
+			#ifdef CONFIG_NUMA
+			if (node != NUMA_NO_NODE && page_to_nid(sp) != node)
+				continue;
+			#endif
+		
+			if(&sp->list == wall )
+				break;
+			
+			if (sp->units - SLOB_UNITS(size) < 0  ){
+				//list_move_tail(&sp->list, slob_list);
+				continue;
+			}
+
+			if (sp->units - SLOB_UNITS(size) < min) {
+				min = sp->units - SLOB_UNITS(size);
+				best = sp;
+				if (min==prev_min)//den uparxei kati kalitero
+					break;
+			}
+			
+		}
+		
+		prev_min = min;
+		
+		
+		if (min != 10000000) {
+			b = slob_page_alloc(best, size, align);
+			if (!b){
+				min = 10000000;
+				list_move_tail(&best->list, wall);
+				wall = &best->list;
+				
+				continue;
+			}
+			else { 
+
+				spin_unlock_irqrestore(&slob_lock, flags);
+				if (unlikely((gfp & __GFP_ZERO) && b))
+					memset(b, 0, size);
+				return b;
+			}
+		}
+		break;
+	}
+		
+	spin_unlock_irqrestore(&slob_lock, flags);
+
+	 Not enough space: must allocate a new page
+	if (!b) {
+		b = slob_new_pages(gfp & ~__GFP_ZERO, 0, node);
+		if (!b)
+			return NULL;
+		sp = virt_to_page(b);
+		__SetPageSlab(sp);
+		
+		spin_lock_irqsave(&slob_lock, flags);
+		sp->units = SLOB_UNITS(PAGE_SIZE);
+		sp->freelist = b;
+		INIT_LIST_HEAD(&sp->list);
+		set_slob(b, SLOB_UNITS(PAGE_SIZE), b + SLOB_UNITS(PAGE_SIZE));
+		set_slob_page_free(sp, slob_list);
+		b = slob_page_alloc(sp, size, align);
+		BUG_ON(!b);
+		spin_unlock_irqrestore(&slob_lock, flags);
+	}
+	if (unlikely((gfp & __GFP_ZERO) && b))
+		memset(b, 0, size);
+	return b;
+}
+#endif
+
+#ifndef BLOCK_BEST_FIT
 static void *slob_page_alloc(struct page *sp, size_t size, int align)
 {
 	slob_t *prev, *cur, *aligned = NULL;
@@ -221,7 +445,6 @@
 
 	for (prev = NULL, cur = sp->freelist; ; prev = cur, cur = slob_next(cur)) {
 		slobidx_t avail = slob_units(cur);
-
 		if (align) {
 			aligned = (slob_t *)ALIGN((unsigned long)cur, align);
 			delta = aligned - cur;
@@ -261,10 +484,9 @@
 			return NULL;
 	}
 }
+#endif
 
-/*
- * slob_alloc: entry point into the slob allocator.
- */
+#ifndef PAGE_BEST_FIT
 static void *slob_alloc(size_t size, gfp_t gfp, int align, int node)
 {
 	struct page *sp;
@@ -273,14 +495,33 @@
 	slob_t *b = NULL;
 	unsigned long flags;
 
-	if (size < SLOB_BREAK1)
+	
+	counter++;
+
+	if (size < SLOB_BREAK1){
 		slob_list = &free_slob_small;
-	else if (size < SLOB_BREAK2)
+		free_mem_small = 0;
+	}
+	else if (size < SLOB_BREAK2){
 		slob_list = &free_slob_medium;
-	else
+		free_mem_medium = 0;
+		
+	}
+	else{
 		slob_list = &free_slob_large;
+		free_mem_large = 0;
+	}
 
 	spin_lock_irqsave(&slob_lock, flags);
+	
+	list_for_each_entry(sp, slob_list, list) { // get statistics, den kostizei poly
+			if (size < SLOB_BREAK1)
+				free_mem_small += sp->units;
+			else if (size < SLOB_BREAK2)
+				free_mem_medium += sp->units;
+			else
+				free_mem_large += sp->units;
+	}
 	/* Iterate through each partially free page, try to find room */
 	list_for_each_entry(sp, slob_list, list) {
 #ifdef CONFIG_NUMA
@@ -333,6 +574,7 @@
 		memset(b, 0, size);
 	return b;
 }
+#endif
 
 /*
  * slob_free: entry point into the slob allocator.
@@ -356,11 +598,14 @@
 
 	if (sp->units + units == SLOB_UNITS(PAGE_SIZE)) {
 		/* Go directly to page allocator. Do not pass slob allocator */
+		
 		if (slob_page_free(sp))
 			clear_slob_page_free(sp);
 		spin_unlock_irqrestore(&slob_lock, flags);
 		__ClearPageSlab(sp);
 		page_mapcount_reset(sp);
+		
+		
 		slob_free_pages(b, 0);
 		return;
 	}
@@ -643,3 +888,15 @@
 {
 	slab_state = FULL;
 }
+
+SYSCALL_DEFINE0(slob_get_total_alloc_mem) {
+	
+	return alloc_mem;
+}
+
+SYSCALL_DEFINE0(slob_get_total_free_mem) {
+
+	return (((free_mem_small+free_mem_medium+free_mem_large)* (SLOB_UNIT)) - SLOB_UNIT + 1);
+}
+
+
diff -ruN linux-3.14.62-orig/security/tomoyo/builtin-policy.h linux-3.14.62-dev/security/tomoyo/builtin-policy.h
--- linux-3.14.62-orig/security/tomoyo/builtin-policy.h	1970-01-01 02:00:00.000000000 +0200
+++ linux-3.14.62-dev/security/tomoyo/builtin-policy.h	2019-04-16 00:59:45.649223469 +0300
@@ -0,0 +1,12 @@
+static char tomoyo_builtin_profile[] __initdata =
+"";
+static char tomoyo_builtin_exception_policy[] __initdata =
+"initialize_domain /sbin/modprobe from any\n"
+"initialize_domain /sbin/hotplug from any\n"
+"";
+static char tomoyo_builtin_domain_policy[] __initdata =
+"";
+static char tomoyo_builtin_manager[] __initdata =
+"";
+static char tomoyo_builtin_stat[] __initdata =
+"";
diff -ruN linux-3.14.62-orig/security/tomoyo/policy/exception_policy.conf linux-3.14.62-dev/security/tomoyo/policy/exception_policy.conf
--- linux-3.14.62-orig/security/tomoyo/policy/exception_policy.conf	1970-01-01 02:00:00.000000000 +0200
+++ linux-3.14.62-dev/security/tomoyo/policy/exception_policy.conf	2019-04-16 00:59:45.633223469 +0300
@@ -0,0 +1,2 @@
+initialize_domain /sbin/modprobe from any
+initialize_domain /sbin/hotplug from any
